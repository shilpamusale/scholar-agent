{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7402b44",
   "metadata": {},
   "source": [
    "##### ScholarAgent: Full Ingestion and Retrieval Test\n",
    "**Objective:** This notebook provides a self-contained workflow to debug our RAG pipeline. It will:\n",
    "1. Ingest the raw PDF documents.\n",
    "2. Create the ChromaDB vector store.\n",
    "3. Immediately test the retriever with a simple query to confirm it's working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850d00e",
   "metadata": {},
   "source": [
    "#### 2. Imports and Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e98714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa2acc",
   "metadata": {},
   "source": [
    "#### 3. Run the ingestion pipeline\n",
    "The cell below will execute the full data ingestion process:\n",
    "1. Load PDFs from `data/raw`\n",
    "2. Split them into chunks\n",
    "3. Create embeddings and persist the vector store to `data/processed/chroma_db`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04dab7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 02:03:32,338 - src.data_processing.ingestion - INFO - Starting data ingestion pipeline...\n",
      "2025-08-29 02:03:32,339 - src.data_processing.ingestion - INFO - Loading documents from /workspaces/scholar-agent/data/raw...\n",
      "2025-08-29 02:04:03,105 - src.data_processing.ingestion - INFO - Total documents loaded: 415\n",
      "2025-08-29 02:04:03,105 - src.data_processing.ingestion - INFO - Splitting documents into chunks...\n",
      "2025-08-29 02:04:03,151 - src.data_processing.ingestion - INFO - Created 1707 chunks.\n",
      "2025-08-29 02:04:03,152 - src.data_processing.ingestion - INFO - Creating vector store...\n",
      "/workspaces/scholar-agent/src/data_processing/ingestion.py:51: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(model_name=settings.EMBEDDING_MODEL_NAME)\n",
      "/workspaces/scholar-agent/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-29 02:06:25,916 - src.data_processing.ingestion - INFO - Vector store created and persisted at /workspaces/scholar-agent/data/processed/chroma_db\n",
      "2025-08-29 02:06:25,917 - src.data_processing.ingestion - INFO - Data ingestion pipeline finished successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.data_processing.ingestion import run_ingestion_pipeline\n",
    "run_ingestion_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
