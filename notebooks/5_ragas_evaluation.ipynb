{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f074fa5",
   "metadata": {},
   "source": [
    "##### ScholarAgent: Quantitative Evaluation with RAGAS\n",
    "**Objective:** To quantitatively measure the performance of our advanced RAG pipeline using the RAGAS framework. This moves our project from a qualitative demo to a rigorous, research-grade system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc3a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths and environment variables loaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the project root to the Python path.\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Laod environment variables\n",
    "load_dotenv()\n",
    "print(\"Paths and environment variables loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd869850",
   "metadata": {},
   "source": [
    "#### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577c2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/scholar-agent/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspaces/scholar-agent/venv/lib/python3.12/site-packages/ragas/metrics/__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "/workspaces/scholar-agent/venv/lib/python3.12/site-packages/ragas/metrics/__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy,context_precision, context_recall\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "# from ragas.integrations.langchain import LangchainLLM\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from src.rag_pipeline.core import create_rag_chain\n",
    "import configs.settings as settings\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n",
    "\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(f\" WARNING: GOOGLE_API_KEY not found in .env file. RAGAS evaluation may fail. \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181afac6",
   "metadata": {},
   "source": [
    "#### 2. Define the Evaluation Set\n",
    "This is the most critical part of a good evaluation. We need high-quality questions and \"ground truth\" answers that are derived directly from our source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd91341",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What is the core problem with polysemantic neurons?\",\n",
    "    \"How does dictionary learning with sparse autoencoders attempt to solve polysemanticity?\",\n",
    "    \"What is a 'feature' in the context of mechanistic interpretability?\",\n",
    "    ]\n",
    "\n",
    "ground_truth_answers = [\n",
    "    \"The core problem with polysemantic neurons is that they are frequently activated by several completely different types of inputs, making them difficult to interpret and assign a single, clear function to.\",\n",
    "    \"Dictionary learning with sparse autoencoders attempts to solve polysemanticity by decomposing model activations into a larger set of more specific, interpretable features, where each feature corresponds to a single meaningful concept (monosemanticity).\",\n",
    "    \"In mechanistic interpretability, a 'feature' is a specific, human-interpretable variable or concept that a model uses for computation, often represented as a pattern of neuron activations.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0271a",
   "metadata": {},
   "source": [
    "#### 3. Generate Answers with our RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a32563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 12:55:27,194 - src.rag_pipeline.core - INFO - Creating the RAG chain for evaluation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/scholar-agent/src/rag_pipeline/core.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(\n",
      "/workspaces/scholar-agent/src/rag_pipeline/core.py:41: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n",
      "2025-08-30 12:55:34,121 - src.rag_pipeline.core - INFO - Base retriever created successfully.\n",
      "2025-08-30 12:55:34,294 - src.rag_pipeline.core - INFO - Flashrank re-ranker initialized.\n",
      "2025-08-30 12:55:34,295 - src.rag_pipeline.core - INFO - Prompt template created.\n",
      "2025-08-30 12:55:34,309 - src.rag_pipeline.core - INFO - LLM initialized with model: gemini-1.5-flash\n",
      "2025-08-30 12:55:34,310 - src.rag_pipeline.core - INFO - RAG chain for evaluation created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answering: What is the core problem with polysemantic neurons?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 12:55:36,942 - src.rag_pipeline.core - INFO - Re-ranked 20 documents down to 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answering: How does dictionary learning with sparse autoencoders attempt to solve polysemanticity?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 12:55:40,329 - src.rag_pipeline.core - INFO - Re-ranked 20 documents down to 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answering: What is a 'feature' in the context of mechanistic interpretability?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 12:55:42,981 - src.rag_pipeline.core - INFO - Re-ranked 20 documents down to 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer generation complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing RAG chain...\")\n",
    "rag_chain = create_rag_chain()\n",
    "generated_answers = []\n",
    "retrieved_contexts = []\n",
    "# We need to get not just the answer, but also the context that was used to generate it.\n",
    "# We can get this by invoking the chain with a specific structure.\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\" Answering: {question}\")\n",
    "    # The `with_config` allows us to name the run for tracing if needed\n",
    "    response = rag_chain.with_config(run_name=\"test_question_run\").invoke(question)\n",
    "\n",
    "    generated_answers.append(response[\"answer\"])\n",
    "    retrieved_contexts.append([doc.page_content for doc in response[\"context\"]])\n",
    "\n",
    "print(\"Answer generation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11fc672",
   "metadata": {},
   "source": [
    "#### 4. Run the RAGAS Evaluation\n",
    "This is the final step. We'll combine our questions, ground truth answers, generated answers, and retrieved contexts into a dataset and pass it to RAGAS for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1228ad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAGAS evaluation...\n",
      "Running RAGAS evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 12/12 [00:14<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAGAS Evaluation Complete ---\n",
      "{'context_precision': 0.5810, 'context_recall': 1.0000, 'faithfulness': 1.0000, 'answer_relevancy': 0.6829}\n"
     ]
    }
   ],
   "source": [
    "# Combine all the data into a Hugging Face Dataset object\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": generated_answers,\n",
    "    \"contexts\": retrieved_contexts,\n",
    "    \"ground_truth\": ground_truth_answers,\n",
    "    })\n",
    "\n",
    "# Initialize the models RAGAS will use\n",
    "ragas_llm = ChatGoogleGenerativeAI(model=settings.LLM_MODEL_NAME)\n",
    "ragas_embeddings = SentenceTransformerEmbeddings(model_name=settings.EMBEDDING_MODEL_NAME)\n",
    "print(\"Running RAGAS evaluation...\")\n",
    "\n",
    "# gemini_llm = ChatGoogleGenerativeAI(model=settings.LLM_MODEL_NAME)\n",
    "\n",
    "# ragas_llm = LangchainLLM(llm=gemini_llm)\n",
    "# faithfulness.llm = ragas_llm\n",
    "# answer_relevancy.llm = ragas_llm\n",
    "# context_recall.llm = ragas_llm\n",
    "# context_precision.llm = ragas_llm\n",
    "\n",
    "print(\"Running RAGAS evaluation...\")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=response_dataset, \n",
    "    metrics=[\n",
    "        context_precision,  # Evaluates the retriever\\n\",\n",
    "        context_recall,     # Evaluates the retriever\\n\",\n",
    "        faithfulness,       # Evaluates the generator\\n\",\n",
    "        answer_relevancy,   # Evaluates the generator\\n\",\n",
    "    ]\n",
    "    ,llm=ragas_llm\n",
    "    ,embeddings=ragas_embeddings\n",
    ")\n",
    "\n",
    "print(\"--- RAGAS Evaluation Complete ---\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4d063",
   "metadata": {},
   "source": [
    "#### 5. Analyze the Results\n",
    "The output above gives us a dictionary of scores. A score of `1.0` is perfect, and a score of `0.0` is the worst. We are looking for high scores in all categories, especially:\n",
    "\n",
    "- **`faithfulness`**: How factually accurate is the answer based *only* on the provided context? This is a key metric for reducing hallucinations.\n",
    "- **`context_recall`**: Did the retriever find all the relevant information needed to answer the question?\n",
    "- **`answer_relevancy`**: Is the answer actually relevant to the question being asked?\n",
    "- **`context_precision`**: Is the retrieved context precise and to the point, or does it contain a lot of irrelevant noise?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58403e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
