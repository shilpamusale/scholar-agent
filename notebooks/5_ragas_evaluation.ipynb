{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f074fa5",
   "metadata": {},
   "source": [
    "##### ScholarAgent: Quantitative Evaluation with RAGAS\n",
    "**Objective:** To quantitatively measure the performance of our advanced RAG pipeline using the RAGAS framework. This moves our project from a qualitative demo to a rigorous, research-grade system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc3a763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the project root to the Python path.\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Laod environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd869850",
   "metadata": {},
   "source": [
    "#### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577c2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy,context_precision, context_recall\n",
    "\n",
    "from src.rag_pipeline.core import create_rag_chain\n",
    "\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(f\" WARNING: GOOGLE_API_KEY not found in .env file. RAGAS evaluation may fail. \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181afac6",
   "metadata": {},
   "source": [
    "#### 2. Define the Evaluation Set\n",
    "This is the most critical part of a good evaluation. We need high-quality questions and \"ground truth\" answers that are derived directly from our source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd91341",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What is the core problem with polysemantic neurons?\",\n",
    "    \"How does dictionary learning with sparse autoencoders attempt to solve polysemanticity?\",\n",
    "    \"What is a 'feature' in the context of mechanistic interpretability?\",\n",
    "    ]\n",
    "\n",
    "ground_truth_answers = [\n",
    "    \"The core problem with polysemantic neurons is that they are frequently activated by several completely different types of inputs, making them difficult to interpret and assign a single, clear function to.\",\n",
    "    \"Dictionary learning with sparse autoencoders attempts to solve polysemanticity by decomposing model activations into a larger set of more specific, interpretable features, where each feature corresponds to a single meaningful concept (monosemanticity).\",\n",
    "    \"In mechanistic interpretability, a 'feature' is a specific, human-interpretable variable or concept that a model uses for computation, often represented as a pattern of neuron activations.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0271a",
   "metadata": {},
   "source": [
    "#### 3. Generate Answers with our RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7a32563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 22:54:07,267 - src.rag_pipeline.core - INFO - Creating the RAG chain with re-ranking...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 22:54:07,601 - src.rag_pipeline.core - INFO - Base retriever created successfully.\n",
      "2025-08-29 22:54:07,749 - src.rag_pipeline.core - INFO - Prompt template created.\n",
      "2025-08-29 22:54:07,751 - src.rag_pipeline.core - INFO - LLM initialized with model: gemini-1.5-flash\n",
      "2025-08-29 22:54:07,752 - src.rag_pipeline.core - INFO - RAG chain with re-ranking created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answering: What is the core problem with polysemantic neurons?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# The `with_config` allows us to name the run for tracing if needed\u001b[39;00m\n\u001b[32m     11\u001b[39m     response = rag_chain.with_config(run_name=\u001b[33m\"\u001b[39m\u001b[33mtest_question_run\u001b[39m\u001b[33m\"\u001b[39m).invoke(question)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     generated_answers.append(\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manswer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     14\u001b[39m     retrieved_contexts.append([doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m]])\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnswer generation complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "print(\"Initializing RAG chain...\")\n",
    "rag_chain = create_rag_chain()\n",
    "generated_answers = []\n",
    "retrieved_contexts = []\n",
    "# We need to get not just the answer, but also the context that was used to generate it.\n",
    "# We can get this by invoking the chain with a specific structure.\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\" Answering: {question}\")\n",
    "    # The `with_config` allows us to name the run for tracing if needed\n",
    "    response = rag_chain.with_config(run_name=\"test_question_run\").invoke(question)\n",
    "\n",
    "    generated_answers.append(response[\"answer\"])\n",
    "    retrieved_contexts.append([doc.page_content for doc in response[\"context\"]])\n",
    "\n",
    "print(\"Answer generation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11fc672",
   "metadata": {},
   "source": [
    "#### 4. Run the RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hugging Face Dataset from our results\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": generated_answers,\n",
    "    \"contexts\": retrieved_contexts,\n",
    "    \"ground_truth\": ground_truth_answers,\n",
    "    })\n",
    "\n",
    "print(\"Running RAGAS evaluation...\")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=response_dataset, \n",
    "    metrics=[\n",
    "    context_precision,  # Evaluates the retriever\\n\",\n",
    "    context_recall,     # Evaluates the retriever\\n\",\n",
    "    faithfulness,       # Evaluates the generator\\n\",\n",
    "    answer_relevancy,   # Evaluates the generator\\n\",\n",
    "    ],\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- RAGAS Evaluation Complete ---\")\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
